{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Engineering Python Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to delete existent parquet files and re-produce the conversion.\n",
    "!rm -rf pyarrow/basic/\n",
    "!rm -rf fastparquet/basic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq \n",
    "import fastparquet as fsp \n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df201 = pd.read_csv('weather.20160201.csv', parse_dates=[\"ObservationDate\"])\n",
    "df301 = pd.read_csv(\"weather.20160301.csv\", parse_dates=['ObservationDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(93255, 15) (101442, 15)\n"
    }
   ],
   "source": [
    "print(df201.shape, df301.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parquet supported packages\n",
    "There is two widely used package within the python ecosystem which is pyarrow and fastparquest. Pyarrow is c++ implementation of the Parquet file format that allows efficient implementation in python. On the other hand fastparquet is python based implementation of the Parquet file takes advantage of the numbas JIT compiler. Despite the fact pandas library allows conversion between file formats to Parquet using either of these packages, more granular customization for the I/O operations can be achieve using the packages explicitly. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pyarrow conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl201 = pa.Table.from_pandas(df201, preserve_index=False)\n",
    "tbl301 = pa.Table.from_pandas(df301, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_to_dataset(tbl201, root_path=\"pyarrow/basic\")\n",
    "pq.write_to_dataset(tbl301, root_path=\"pyarrow/basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "37.2 ms ± 6.22 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "#Only read the necesary columns\n",
    "pyarrow_tbl = pq.ParquetDataset(\"pyarrow/basic\")\n",
    "full_df = pyarrow_tbl.read(columns=[\"ObservationDate\",\n",
    "\"ScreenTemperature\",\n",
    "\"Region\"]).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyarrow_tbl = pq.ParquetDataset(\"pyarrow/basic\")\n",
    "full_df = pyarrow_tbl.read(columns=[\"ObservationDate\",\n",
    "\"ScreenTemperature\",\n",
    "\"Region\"]).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_df) == len(df201) + len(df301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['ObservationDate', 'ScreenTemperature', 'Region'], dtype='object')"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ObservationDate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>147768</th>\n      <td>2016-03-17</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "       ObservationDate\n147768      2016-03-17"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.loc[full_df[\"ScreenTemperature\"] == full_df[\"ScreenTemperature\"].max(), [\"ObservationDate\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ObservationDate</th>\n      <th>ScreenTemperature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>147768</th>\n      <td>2016-03-17</td>\n      <td>15.8</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "       ObservationDate  ScreenTemperature\n147768      2016-03-17               15.8"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.loc[full_df[\"ScreenTemperature\"] == full_df[\"ScreenTemperature\"].max(), [\"ObservationDate\", \"ScreenTemperature\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Region</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>147768</th>\n      <td>Highland &amp; Eilean Siar</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                        Region\n147768  Highland & Eilean Siar"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.loc[full_df[\"ScreenTemperature\"] == full_df[\"ScreenTemperature\"].max(), [\"Region\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fastparquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsp.write(filename=\"fastparquet/basic\", data=df201, file_scheme='hive', write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsp.write(filename=\"fastparquet/basic/\", data=df301, file_scheme=\"hive\", write_index=False, append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdata_max = fsp.ParquetFile(\"fastparquet/basic/_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_temp_list = mdata_max.statistics['max']['ScreenTemperature']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = max_temp_list.index(max(max_temp_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Read only the relevant file and relevant columns wilth the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "72.3 ms ± 12.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "related_file = fsp.ParquetFile(glob.glob(\"fastparquet/basic/*\")[max_index]).to_pandas(columns=['ObservationDate', 'ScreenTemperature', 'Region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_file = fsp.ParquetFile(glob.glob(\"fastparquet/basic/*\")[max_index]).to_pandas(columns=['ObservationDate', 'ScreenTemperature', 'Region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ObservationDate</th>\n      <th>ScreenTemperature</th>\n      <th>Region</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>147768</th>\n      <td>2016-03-17</td>\n      <td>15.8</td>\n      <td>Highland &amp; Eilean Siar</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "       ObservationDate  ScreenTemperature                  Region\n147768      2016-03-17               15.8  Highland & Eilean Siar"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_file.loc[related_file['ScreenTemperature'] == related_file['ScreenTemperature'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimization related issues"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Both of these libraries allow finer grained implementations such as writing files in desired row groups based on the number of rows and writing partitioned files based on the partition columns. \n",
    "For the files we have it wouldn't be useful to have any partitioned implementations because the file sizes are relatively small. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Avoiding a lot of small files\n",
    "It is advisable to avoiding to write too many small files because every parquet file includes metadata in the footer of the file. Hence having too many small files lead to writing similar metadata over and over again and using more storage than needed."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Avoiding very large files\n",
    "It is also advisable to avoid have a very large file even though its a high dimensional data and file needs to be read only selected columns. Because of the very large file can have very long footer metadata because of the record of the unique values stored in the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}